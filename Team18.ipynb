{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3d2f91-1f6f-4b8a-be8c-eb673e74ef6d",
   "metadata": {},
   "source": [
    "# CS4248 NLP Project Team 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0af41-f2e7-40e9-850a-a08f76908f4e",
   "metadata": {},
   "source": [
    "## Installing necessary libraries\n",
    "\n",
    "*Note*: Put libraries that need to be installed with `!pip install LIBRARY` so that we can ensure consistency in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8a95b-3cc9-48bc-b669-479e3a06b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow\n",
    "!pip install shap\n",
    "!pip install nltk\n",
    "!pip install textstat\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46d2bc-9a4b-47b1-829e-c7db818d13c0",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a07b5-7b7b-4b4d-a60c-88bd9518a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_FILEPATH = \"raw_data/fulltrain.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_FILEPATH, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bb89a-3e1d-434f-8904-69d9220a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[1]\n",
    "y = df[0]\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acb502-671c-441a-b82c-29a5edd337e4",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48003e8a-7bfb-42fa-a0d2-e5244930d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "value_counts = df[0].value_counts()\n",
    "value_counts.plot(kind='bar')\n",
    "plt.xlabel('Classification')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Breakdown of Text Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d2eaf-d02d-43e8-b295-a361b2ecada7",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d37877-303a-494d-b7b0-b53f7b2c22d7",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f1ebe-18d2-4264-8658-90e5c8e9b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "def normalize(data):\n",
    "    return (data-data.mean())/data.std()\n",
    "\n",
    "def count_tokens(sentence):\n",
    "    return len(word_tokenize(sentence))\n",
    "\n",
    "def get_polarity(sentence):\n",
    "    return TextBlob(sentence).sentiment.polarity\n",
    "\n",
    "def get_subjectivity(sentence):\n",
    "    return TextBlob(sentence).sentiment.subjectivity\n",
    "\n",
    "def add_features(data):\n",
    "    \n",
    "    # Add features\n",
    "    token_count = data.apply(lambda x: count_tokens(x))\n",
    "    readability = data.apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "    polarity = data.apply(lambda x: get_polarity(x))\n",
    "    subjectivity = data.apply(lambda x: get_subjectivity(x))\n",
    "    \n",
    "    # Normalize features\n",
    "    token_count = normalize(token_count)\n",
    "    readability = normalize(readability)\n",
    "    polarity = normalize(polarity)\n",
    "    subjectivity = normalize(subjectivity)\n",
    "    \n",
    "    return pd.concat([token_count, readability, polarity, subjectivity], axis=1)\n",
    "\n",
    "\n",
    "train = pd.read_csv('balancedtest.csv', header=None)\n",
    "\n",
    "x_train = train[train.columns[1]]\n",
    "y_train = train[train.columns[0]]\n",
    "\n",
    "train_features = add_features(x_train)\n",
    "\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91081f05-7bb2-45c8-a463-9fc9f52c9a9f",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada1293-7ce1-43fb-a37f-b4e815a0014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "tfidf_matrix = vectorizer.fit_transform(X)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc08365-4dc1-443c-af24-b0439fda9f21",
   "metadata": {},
   "source": [
    "## Settings for train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad85677-cb12-40ac-ad2d-06a66df162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "## Note: Change this to fit the algorithm below\n",
    "# X are the features\n",
    "X = tfidf_df\n",
    "# y are the outputs\n",
    "y = y\n",
    "# test_size is the size of the test (0 < test_size < 1)\n",
    "test_size = 0.2\n",
    "# seed for random split\n",
    "seed = 40\n",
    "## End of Note\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb7c17-5286-4121-869e-2f597d956fcd",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bc350-c65f-4b53-91de-c616d5c9beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluation_metrics = [(\"Accuracy\", accuracy_score), (\"Confusion Matrix\", confusion_matrix)]\n",
    "\n",
    "for evaluation_metric_name, evaluation_metric_func in evaluation_metrics:\n",
    "    print(f\"{evaluation_metric_name}:\\n{evaluation_metric_func(y_test, y_pred)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a2df9-dfd6-4e9c-b1d3-53d4608b2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [(\"Accuracy\", accuracy_score), (\"Confusion Matrix\", confusion_matrix)]\n",
    "\n",
    "for evaluation_metric_name, evaluation_metric_func in evaluation_metrics:\n",
    "    print(f\"{evaluation_metric_name}:\\n{evaluation_metric_func(y_test, y_pred)}\")\n",
    "f1_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fbc6a-422f-4b00-88c1-05b735d66593",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002416e-7a0c-4262-a948-984db058960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(image_height, image_width, num_channels)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b71d99-4b32-4f2e-afdf-9ebecf2b3b70",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db360e1-233c-40ea-ba87-ebb8fc6d06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "X_train_array = X_train.to_numpy()\n",
    "X_test_array = X_test.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "y_test_array = y_test.to_numpy()\n",
    "\n",
    "num_samples, num_features = X_train_array.shape\n",
    "timesteps = 1  # You may need to adjust this depending on your data\n",
    "\n",
    "X_train_reshaped = X_train_array.reshape(num_samples, timesteps, num_features)\n",
    "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0], timesteps, X_test_array.shape[1])\n",
    "\n",
    "\n",
    "# create model here\n",
    "model = tf.keras.Sequential([\n",
    "    SimpleRNN(units=32, input_shape=(timesteps, num_features)),\n",
    "    Dense(units=4)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32)\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbe374-2fca-4b13-adb8-c97d2208518e",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cfa48-a6bf-4390-8fea-4139b44ead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your pandas DataFrame with a column named 'text' containing the sentences\n",
    "# and a column named 'label_column' containing the labels\n",
    "# Example DataFrame:\n",
    "# df = pd.DataFrame({'text': [\"This is sentence 1.\", \"Another sentence.\", \"Yet another sentence.\"],\n",
    "#                    'label_column': [0, 1, 1]})\n",
    "\n",
    "X = df[1].values\n",
    "y = df[0].values\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "max_sequence_length = 100 # max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Convert to Numeric\n",
    "X = padded_sequences\n",
    "y = y\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM Model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100  # Adjust as needed\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=128, implementation=2))  # Disable CuDNN\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a data generator\n",
    "def data_generator(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    while True:\n",
    "        indices = np.random.permutation(np.arange(num_samples))\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "# Train Model with Data Generator\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "train_generator = data_generator(X_train, y_train, batch_size)\n",
    "\n",
    "# Train Model with Generator\n",
    "model.fit(train_generator, epochs=10, steps_per_epoch=steps_per_epoch, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"F1 score (macro):\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35037039-7f05-47bb-8cbd-fd68d0679300",
   "metadata": {},
   "source": [
    "# Evaluation [unable to see other feature names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d11451-f66d-44db-974d-f63a16aa34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test[50:100])\n",
    "shap.summary_plot(shap_values, X_test[50:100], max_display=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
